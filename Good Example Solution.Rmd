---
title: "Practical ML"
author: "José Antonio García Ramirez"
date: "January 19, 2017"
output: 
  md_document:
  variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```




## Get the Data 

Since the Train and Test sets are defined in this [exercise](https://www.coursera.org/learn/practical-machine-learning/peer/R43St/prediction-assignment-writeup), we will not say anything about the bias that can contain the sample that forms the Train set but even though in an ideal case, with enough computer equipment, we would like to use the one-leave-out method to train the models, since the final part of the exercise consists of predicting 20 observations, due to run-time issues. It was decided to use the 10-times validation, which is faster than one-leave-out validation and also presents less risk in overtraining the models with the Trainset.


```{r getdata}
Train <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
Test <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
```

## Preprocess and pruning
Then we get the datasets, directly from the web addresses, and then we notice that many of the variables containing both datasets present a considerable amount of NA and we remove those variables with little information

```{r removeNA}
library(ggplot2)
NApercentage <- function(column)
{
	return(mean(is.na(column)))
}
listNA <- apply(Train, 2, NApercentage)
listNA
```

It is decided to eliminate those variables where more than 50% of their contents are NA, and after a manual revision (with the summary function), the "read.csv" function identifies the character "# DIV / 0!" In some columns what originated that those that contain it are read as factors instead of being numerical, all of them also are little informative because they contain a large percentage of NA, also these variables were discarded including variables with temporal information.

```{r removeNA2}
removeVar <- listNA[listNA > .5] 
Vars <- setdiff(colnames(Train), names(removeVar))
#summary(Train[,Vars])
```



```{r removeNA3}
removeDIV <- c("X", "raw_timestamp_part_1", "raw_timestamp_part_2",
			   "cvtd_timestamp", "kurtosis_roll_belt", "kurtosis_picth_belt",
			   "kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1",
			   "skewness_yaw_belt", "max_yaw_belt", "min_yaw_belt",
			   "amplitude_yaw_belt", "kurtosis_roll_arm", "kurtosis_picth_arm",
			   "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm",
			   "skewness_yaw_arm", "kurtosis_roll_dumbbell",
			   "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", 
			   "skewness_roll_dumbbell", "skewness_pitch_dumbbell",
			   "skewness_yaw_dumbbell", "max_yaw_dumbbell", 
			   "min_yaw_dumbbell", "amplitude_yaw_dumbbell",
			   "kurtosis_roll_forearm", "kurtosis_picth_forearm",
			   "kurtosis_yaw_forearm", "skewness_roll_forearm",
			   "skewness_pitch_forearm", "skewness_yaw_forearm",
			   "max_yaw_forearm", "min_yaw_forearm", "amplitude_yaw_forearm")
Vars <- setdiff(Vars, removeDIV)			   
```

At this point in the Train dataset set there are no NA values so no pruning is required. 

```{r naomit}
sum(is.na(Train[,Vars]))
``` 

It is important to note the distribution of the classes in our dataset, as you can see in table 1, class 'A' has approximately 28% of the cases, this induces a lower bound on the performance of the classification algorithms that we will try.

```{r firstaprox}
library(pander)
a <- table(Train$classe)/ dim(Train)[1]
pander(round(a, 2))
```
Table 1: _classe_'s distribution, target variable, in the training dataset.  


## Models selection

Given that 55 features are available, some of which are numeric and other factors, to predict one of type factor, variable _classe_,  we are faced with a classification problem rather than a regression problem.

Given that the content of most of the available variables is uncertain, three algorithms will be used that are not sensitive to transformations of the variables( incluying centering and scaling):

  * Quadratic discriminant analysis (QDA)
  * Random forest
  * Support vector machine (SVM)

So, we train the models, first the QDA. A LDA model was also trained but its accuracy is approximately 0.77 and for it was discarded.

```{r QDA }
library(caret)
controlTrain <- trainControl(method = "cv", number = 2, allowParallel = TRUE)
QDA <- train(classe~. , data = Train[, Vars], method = "qda",
			 trControl = controlTrain)
QDAClases <- predict(QDA, newdata = Train[, Vars])
confusionMatrix(QDAClases, Train$classe)
```
  


Surprisingly, the SVM algorithm has less accuracy than the QDA

```{r SV}
SV <- train(classe ~ ., data = Train[, Vars], method = "svmLinear",
			trControl = controlTrain )
SVClases <- predict(SV, newdata = Train[, Vars])
confusionMatrix(SVClases, Train$classe)
```


And even more surprisingly Random Forest classifies phenomenally.
  
```{r RF}
RF <- train(classe ~ ., data = Train[, Vars], method = "rf",
			trControl = controlTrain)
RFClases <- predict(RF, newdata = Train[, Vars])
confusionMatrix(RFClases, Train$classe)
```

In view of the above, the fabulous performance of RF, it is decided to use the predictions of the SVM and QDA models and then give them as inputs to RandomForest, not to overtrain. The assembled model trained on the Train data set is as follows:

```{r rffinal}
dataTrain <- data.frame(SVM = SVClases, QDA = QDAClases, classe = Train$classe )
RF.finalTrain  <- train(classe ~ ., data = dataTrain, method = "rf")
RF.final <- predict(RF.finalTrain, newdata = dataTrain) 
confusionMatrix(RF.final, Train$classe)
```

So the predictions for the 20 different test cases are the following:

```{r future}
library(knitr)
test.SV <- predict(SV, Test)
test.QDA <- predict(QDA, Test)
test.final <- data.frame(SVM = test.SV, QDA = test.QDA,
						 id = Test$problem_id)
test.final$Output <- predict(RF.finalTrain, test.final)
kable(test.final[,c("id", "Output")])
```


